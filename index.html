<!DOCTYPE html>
<html>
	<head>
		<title>Urmish Thakker</title>
	</head>
	<body>
		<div class="container">
    		<div class="blurb">
        		<h2>About me</h2>
				<p>[<a href="https://scholar.google.com/citations?user=-GPPICQAAAAJ&hl=en">Google Scholar</a>][<a href="https://www.linkedin.com/in/urmishthakker/">LinkedIn</a>][<a href="https://github.com/Urmish">Github</a>][<a href="https://twitter.com/UrmishThakker">Twitter</a>]<br>I am a Senior Research Engineer with the ML Research Lab at Arm. I am working at enabling efficient execution of Deep Learning (DL) on small devices. I have investigated structured and tensor decomposition algorithms to compress NN, training hardware friendly RNN Cells, scheduling RNNs on multicore CPU, developing accelerators for CNNs and benchmarking DL applications to isolate performance bottlenecks. <br><br>Prior to Arm, I was at University of Wisconsin Madison pursuing a Masters degree in Computer Science. My research work focused on predicting GPU Speedup of an application using Decision Trees and developing accelerator for classical computer vision applications.<br><br> In my previous employement, I have worked as a performance architect for indirect branch predictors at AMD and as a verification and design engineer for memory controllers, H.264 video encoder decoder and neural network accelerator at Texas Instruments. </p>
				<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
        		<h2>Education</h2>
				<ol>
  					<li>University of Wisconsin Madison (2014-2016) <ul> <li>Masters in Computer Science</li></ul></li>
  					<li>BITS Pilani (2008-2012) <ul><li> B.E. Electrical and Electronics Engineering</li></ul></li>
				</ol>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
        		<h2>Industry Experience</h2>
				<ol>
					<li>Senior Research Engineer, ML Research Lab, Arm (July 2016 - Present)</li>
					<ul>
						<li>Improved execution of RNNs and FC layers on Arm Hardware - Compression techniques, Software libraries, Software tools</li>
						<li>Exploration of Hardware Accelerators for CNNs</li>
						<li>Performance benchmarking of Deep Learning workloads</li>
					</ul>
					<li>Performance Architecture Intern, AMD (May 2015 - July 2015)</li>
					<ul>
						<li>Developed a state of the art indirect brnach predictor that provided better accuracy than similar techniques in Academia</li>
						<li>The design was incorporated in AMD Servers that were out in the market around 2018/2019</li>
					</ul>
					<li>Design Engineer, Kilby Labs, Texas Instruments (Jan 2014 - Jun 2014)</li>
					<ul>
						<li>Exploration of a neural network accelrator for Industry IoT use case</li>
					</ul>
					<li>Design Engineer, OMAP Group, Texas Instruments (July 2012 - Dec 2014)</li>
					<ul>
						<li>Design and Verification of Memory Controllers, Processor SubSystem, ECC Memories and H.264 Video Encoder/Decoder</li>
					</ul>
					<li>Software Engineering Intern, Broadcom (Jan 2012 - May 2012)</li>
					<li>Research Intern, Robotics lab, IIIT Hyderabad (May 2011 - July 2011)</li>
				</ol>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
        		<h2>Publications</h2>
				<ol reversed>
					<li> <i> Improving accuracy of neural networks compressed using fixed structures via doping </i> <br> <b> Urmish Thakker </b>, Ganesh Dasika, Paul Whatmough, Matthew Mattina <br> 
					tinyML Summit 2020
					[<a href="https://www.tinymlsummit.org/">Link to Summit</a>]<br>
					<li> <i> Aggressive Compression of MobileNets Using Hybrid Ternary Layers </i> <br> Dibakar Gope, Jesse Beu, <b> Urmish Thakker </b>, and Matthew Mattin <br> 
					tinyML Summit 2020
					[<a href="https://www.tinymlsummit.org/">Link to Summit</a>]<br>
					<br />
					<li> <i> Compressing RNNs for IoT devices by 15-38x using Kronecker Products </i> <br> <b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Chu Zhou, Igor Fedorov, Ganesh Dasika and Matthew Mattina <br> 
					[<a href="https://arxiv.org/abs/1906.02876">arxiv pre-print</a>] <br>
					<br />
					<li> <i>Pushing the limits of RNN Compression</i> (<b>NeurIPS-EMC2 2019</b>)<br><b>Urmish Thakker</b>, Igor Fedorov, Jesse Beu, Dibakar Gope, Chu Zhou, Ganesh Dasika and Matthew Mattina <br> 
5th Workshop on Energy Efficient Machine Learning and Cognitive Computing, Co-located with the 33rd Conference on Neural Information Processing Systems (NeurIPS), Dec. 2019. <br>
					Links [<a href="https://www.emc2-workshop.com/neurips-19">Workshop</a>][<a href="https://arxiv.org/abs/1910.02558">Paper</a>]<br>
					<br />
					<li> <i>Skipping RNN State Updates without Retraining the Original Model*</i> (<b>SenSys-ML 2019</b>)<br>
					Jin Tao, <b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu <br> 
					1st Workshop on Machine Learning on Edge in Sensor Systems (Sensys-ML), Co-located with 17th ACM Conference on Embedded Networked Sensor Systems (SenSys 2019), Nov. 2019<br>
					Links [<a href="https://sensysml.github.io/index">Workshop</a>][<a href="https://dl.acm.org/citation.cfm?id=3362965">Paper</a>]<br>
					*Won the best paper award<br>
					<br />
					<li> <i>Run-Time Efficient RNN Compression for Inference on Edge Device</i> (<b>ISCA-EMC2 2019</b>)<br>
					<b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Ganesh Dasika and Matthew Mattina <br>
					4th Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2), Co-located with the 46th Int. Symp on Computer Architecture (ISCA), Jun. 2019. <br>
					Links [<a href="https://www.emc2-workshop.com/isca-19">Workshop</a>][<a href="https://arxiv.org/abs/1906.04886">Paper</a>]<br>
					<br />
					<li> <i>A Static Analysis-based Cross-Architecture Performance Prediction Using Machine Learning </i> (<b>ISCA-AIDArc 2019</b>)<br>
					Newsha Ardalani, <b>Urmish Thakker</b>, Aws Albarghouthi, Karu Sankaralingam <br>
					2nd International Workshop on AI-assisted Design for Architecture co-located with 46th Int. Symposium on Computer Architecture (ISCA), Jun. 2019<br>
					Links [<a href="https://eecs.oregonstate.edu/aidarc/">Workshop</a>][<a href="https://arxiv.org/abs/1906.07840">Paper</a>]<br>
					<br />
					<li> <i>RNN Compression using Hybrid Matrix Decomposition</i> (<b>tinyML 2019</b>)<br> 
					<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Dibakar Gope, and Matthew Mattina <br>
					<b>tinyML Summit</b>, Mar. 2019. <br>
					Links [<a href="https://tinymlsummit.org/2019/">Workshop</a>][<a href="https://tinymlsummit.org/2019/abstracts/Thakker_Urmish_poster.pdf">Paper</a>]<br>
					<br />
					<li> <i>Measuring scheduling efficiency of RNNs for NLP applications</i> (<b>ISPASS-Fasthpath 2019</b>)<br>
					<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Matthew Mattina <br>
					6th edition of International Workshop on Performance Analysis of Machine Learning Systems (Fastpath) co-located with IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2019. <br>
					Links [<a href="https://researcher.watson.ibm.com/researcher/view_group.php?id=9888">Workshop</a>][<a href="https://arxiv.org/abs/1904.03302">Paper</a>]<br>
					<br />
				</ol>
			<hr>
			<h2>Under Review</h2>
				<ol>
					<li> Compressing recurrent neural networks without compromising inference run-time (1st Author)</li>
					<li> Compressing RNNs for IoT devices by 15-38x using Kronecker Products (1st Author)</li>
					<li> Ternary MobileNets via Per-Layer Hybrid Filter Banks (3rd Author)</li>
					<li> Improving accuracy of neural networks compressed using fixed structures via doping </li>
					
				</ol>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
        		<h2>Professional Service</h2>
				<ul>
					<h3>Program Committee</h3>
  					<li>Conference</li>
					<ul>
						<li><a href="https://community.arm.com/developer/research/b/articles/posts/arm-research-summit-2019-catch-up-and-highlights?utm_source=linkedin&utm_medium=social&utm_campaign=2019_eda-other-research_mk07-3_search_arm&utm_term=research-summit-summary&utm_content=blog">Arm Research Summit (2019)</a> </li>
						<li><a href="https://community.arm.com/developer/research/b/articles/posts/catch-up-on-the-arm-research-summit-2018?_ga=2.188913901.797159494.1571152939-403256752.1565579579">Arm Research Summit (2018)</a></li>
					</ul>
  					<li>Workshops
    					  <ul>
						  <li><a href="https://sites.google.com/view/sustainlp2020/home?authuser=0">EMNLP-SustaiNLP (2020)</a></li>
    					  </ul>
  					</li>
					<h3>Reviewer</h3>
  					<li>Journal</li>
					  <ul>
     						<li>IEEE Journal of Selected Topics in Signal Processing (2019) </li>
						<li><a href="https://dl.acm.org/citation.cfm?id=J967">ACM Journal on Emerging Technologies in Computing Systems (2018)</a></li>
    					  </ul>
					  <li>Workshops
    					  <ul>
						  <li><a href="https://www.emc2-workshop.com/neurips-19">NeurIPS-EMC2 (2019)</a></li>
    					  </ul>
  					</li>					   
				</ul>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
			<h2>Media articles</h2>
				<ul>
					<li> <a href="https://community.arm.com/developer/research/b/articles/posts/skipping-rnn-state-updates-without-retraining-the-original-model">Arm Research Blog on Skip-RNN work</a> </li>
				</ul>
			<hr>
        		<h2>Media coverage of work</h2>
				<ul>
					<li> <a href="https://community.arm.com/developer/research/b/articles/posts/taking-constrained-ml-to-the-next-level">Arm Research Blog on Tiny ML work</a> </li>
				</ul>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
	</body>
</html>
