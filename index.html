<!DOCTYPE html>
<html>
	<head>
		<title>Urmish Thakker</title>
	</head>
	<body>
		<div class="container">
    		<div class="blurb">
        		<h2>About me</h2>
				<p>[<a href="https://scholar.google.com/citations?user=-GPPICQAAAAJ&hl=en">Google Scholar</a>][<a href="https://www.linkedin.com/in/urmishthakker/">LinkedIn</a>][<a href="https://github.com/Urmish">Github</a>][<a href="https://twitter.com/UrmishThakker">Twitter</a>]<br>I am a Deep Learning Researcher and Engineer with a passion for using AI to improve everyday lives. I firmly believe that for Deep Learning to have a large scale impact across the globe, these models need to be pushed into devices with small form factors - from mobile phones to microcontrollers. Pushing DL into these devices will truly democratize these technologies by allowing people with limited connectivity to avail its benefit. This specific type of democratization also enables the data to be processed locally, allowing people to avail the benefits of DL without compromising on their right to privacy.<br><br>Currently, I am a Senior Research Engineer with the ML Research Lab at Arm working at enabling efficient execution of Deep Learning (DL) workloads on small devices. My work has led to filing of multiple patents, publications at various venues, Arm ML Software Library improvements and development of new products at Arm. Specifically, I have explored model compression techniques (structured matrices, tensor decomposition, quantization and pruning), faster inference libraries, applied ML for faster inference and CNN Hardware Accelerators. The work around CNN hardware accelerator was the first effort within Arm to explore a new product for the DNN market in the embedded domain. The efforts in this project was one of the catalyst that led to the creation of Arm's NN Accelerator products. Finally, I have also led the efforts to benchmark ML Workloads on Arm platform to isolate performance bottlenecks. <br><br> In my previous employment, I have worked as a performance architect for indirect branch predictors at AMD. The idea and design of the predictor showed significant benefits and is part of the AMD Server Processors. I have also worked as a verification and design engineer for memory controllers, H.264 video encoder decoder and neural network accelerator at Texas Instruments. <br><br>I did my masters at University of Wisconsin Madison pursuing a degree in Computer Science. My research work focused on predicting GPU Speedup of an application using Decision Trees and developing accelerator for classical computer vision applications.</p>
				<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
        		<h2>Education</h2>
				<ol>
  					<li>University of Wisconsin Madison (2014-2016) <ul> <li>Masters in Computer Science</li></ul></li>
  					<li>BITS Pilani (2008-2012) <ul><li> B.E. Electrical and Electronics Engineering</li></ul></li>
				</ol>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
        		<h2>Industry Experience</h2>
				<ol>
					<li>Senior Research Engineer, ML Research Lab, Arm (July 2016 - Present)</li>
					<ul>
						<li>Improved execution of RNNs and FC layers on Arm Hardware - Compression techniques, Software libraries, Software tools</li>
						<li>Exploration of Hardware Accelerators for CNNs</li>
						<li>Performance benchmarking of Deep Learning workloads</li>
						<li>Identifying university collaboration opportunities</li>
						<li>Arm Research's representative at the TinyML Performance Working Group</li>
					</ul>
					<li>Performance Architecture Intern, AMD (May 2015 - July 2015)</li>
					<ul>
						<li>Developed a state of the art indirect brnach predictor that provided better accuracy than similar techniques in Academia</li>
						<li>The design was incorporated in AMD Servers that were out in the market around 2018/2019</li>
					</ul>
					<li>Design Engineer, Kilby Labs, Texas Instruments (Jan 2014 - Jun 2014)</li>
					<ul>
						<li>Exploration of a neural network accelrator for Industry IoT use case</li>
					</ul>
					<li>Design Engineer, OMAP Group, Texas Instruments (July 2012 - Dec 2014)</li>
					<ul>
						<li>Design and Verification of Memory Controllers, Processor SubSystem, ECC Memories and H.264 Video Encoder/Decoder</li>
					</ul>
					<li>Software Engineering Intern, Broadcom (Jan 2012 - May 2012)</li>
					<li>Research Intern, Robotics lab, IIIT Hyderabad (May 2011 - July 2011)</li>
				</ol>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
			<h2>Papers Under Review</h2>
				Conference - 
				<ol>
					<li> Rank and Inference runtime preserving compression technique for RNN based NLP applications (1st Author)</li>
					<li> Compressing RNNs for IoT devices by 15-38x using Kronecker Products (1st Author)</li>
					<li> Ternary MobileNets via Per-Layer Hybrid Filter Banks (3rd Author)</li>
					<li> Federated Learning for Resource-Constrained IoT Devices:Panoramas and State-of-the-art (2nd Author) </li>
					
				</ol>
			<hr>
        		<h2>Publications</h2>
				<h3>Workshop Publications</h3>
				<ol reversed>
					<li> <i> Compressing Language Models using Doped Kronecker Products</i> (<b>On-device Intelligence Workshop</b>)<br> <b> Urmish Thakker </b>, Paul Whatmough, Matthew Mattina, Jesse Beu <br> On-device Intelligence Workshop at Third Conference on Machine Learning and Systems (MLSys), March 2020 <br> Links [<a href="https://mlsys.org/Conferences/2020/Schedule?showEvent=1297">Workshop</a>][<a href="https://arxiv.org/abs/2001.08896">Paper</a>]<br>
					<br />
					<li> <i> Benchmarking TinyML Systems: Challenges and Direction* </i> (<b>Benchmarking Machine Learning Workloads on Emerging Hardware Workshop</b>)<br> Colby Banbury , Vijay Janapa Reddi , Will Fu , Max Lam , Amin Fazel , Jeremy Holleman , Xinyuan Huang , Robert Hurtado , David Kanter , Anton Lokhmotov , David Patterson , Danilo Pau , Jeff Sieracki , Jae-Sun Seo , <b> Urmish Thakkar</b>, Marian Verhelst , Poonam Yadav  <br> First International Workshop on Benchmarking Machine Learning Workloads on Emerging Hardware at Third Conference on Machine Learning and Systems (MLSys), March 2020 <br>*As part of the TinyML Performance Working Group<br>Links [<a href="https://memani1.github.io/challenge20/">Workshop</a>][<a href="">Paper</a>]<br>
					<br />
					<li> <i> Compressing RNNs for IoT devices by 15-38x using Kronecker Products </i> <br> <b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Chu Zhou, Igor Fedorov, Ganesh Dasika and Matthew Mattina <br> 
					[<a href="https://arxiv.org/abs/1906.02876">Technical Report</a>] <br>
					<br />
					<li> <i>Pushing the limits of RNN Compression</i> (<b>NeurIPS-EMC2 2019</b>)<br><b>Urmish Thakker</b>, Igor Fedorov, Jesse Beu, Dibakar Gope, Chu Zhou, Ganesh Dasika and Matthew Mattina <br> 
5th Workshop on Energy Efficient Machine Learning and Cognitive Computing, Co-located with the 33rd Conference on Neural Information Processing Systems (NeurIPS), Dec. 2019. <br>
					Links [<a href="https://www.emc2-workshop.com/neurips-19">Workshop</a>][<a href="https://arxiv.org/abs/1910.02558">Paper</a>]<br>
					<br />
					<li> <i>Skipping RNN State Updates without Retraining the Original Model*</i> (<b>SenSys-ML 2019</b>)<br>
					Jin Tao, <b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu <br> 
					1st Workshop on Machine Learning on Edge in Sensor Systems (Sensys-ML), Co-located with 17th ACM Conference on Embedded Networked Sensor Systems (SenSys 2019), Nov. 2019<br>
					Links [<a href="https://sensysml.github.io/index">Workshop</a>][<a href="https://dl.acm.org/citation.cfm?id=3362965">Paper</a>]<br>
					*Won the best paper award<br>
					<br />
					<li> <i>Run-Time Efficient RNN Compression for Inference on Edge Device</i> (<b>ISCA-EMC2 2019</b>)<br>
					<b>Urmish Thakker</b>, Jesse Beu, Dibakar Gope, Ganesh Dasika and Matthew Mattina <br>
					4th Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2), Co-located with the 46th Int. Symp on Computer Architecture (ISCA), Jun. 2019. <br>
					Links [<a href="https://www.emc2-workshop.com/isca-19">Workshop</a>][<a href="https://arxiv.org/abs/1906.04886">Paper</a>]<br>
					<br />
					<li> <i>A Static Analysis-based Cross-Architecture Performance Prediction Using Machine Learning </i> (<b>ISCA-AIDArc 2019</b>)<br>
					Newsha Ardalani, <b>Urmish Thakker</b>, Aws Albarghouthi, Karu Sankaralingam <br>
					2nd International Workshop on AI-assisted Design for Architecture co-located with 46th Int. Symposium on Computer Architecture (ISCA), Jun. 2019<br>
					Links [<a href="https://eecs.oregonstate.edu/aidarc/">Workshop</a>][<a href="https://arxiv.org/abs/1906.07840">Paper</a>]<br>
					<br />
					
					<li> <i>Measuring scheduling efficiency of RNNs for NLP applications</i> (<b>ISPASS-Fasthpath 2019</b>)<br>
					<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Matthew Mattina <br>
					6th edition of International Workshop on Performance Analysis of Machine Learning Systems (Fastpath) co-located with IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), March 2019. <br>
					Links [<a href="https://researcher.watson.ibm.com/researcher/view_group.php?id=9888">Workshop</a>][<a href="https://arxiv.org/abs/1904.03302">Paper</a>]<br>
					<br />
				<h3>Extended Abstracts/ Posters</h3>
					<li> <i> Improving accuracy of neural networks compressed using fixed structures via doping </i> (<b>tinyML 2020</b>)<br> <b> Urmish Thakker </b>, Ganesh Dasika, Paul Whatmough, Matthew Mattina, Jesse Beu <br> 
					tinyML Summit 2020
					[<a href="https://www.tinymlsummit.org/">Link to Summit</a>]<br>
					<br />
					<li> <i> Aggressive Compression of MobileNets Using Hybrid Ternary Layers </i> (<b>tinyML 2020</b>) <br> Dibakar Gope, Jesse Beu, <b> Urmish Thakker </b>, and Matthew Mattina <br> 
					tinyML Summit 2020
					[<a href="https://www.tinymlsummit.org/">Link to Summit</a>]<br>
					<br />
					<li> <i>RNN Compression using Hybrid Matrix Decomposition</i> (<b>tinyML 2019</b>)<br> 
					<b>Urmish Thakker</b>, Ganesh Dasika, Jesse Beu, Dibakar Gope, and Matthew Mattina <br>
					<b>tinyML Summit</b>, Mar. 2019. <br>
					Links [<a href="https://tinymlsummit.org/2019/">Workshop</a>][<a href="https://tinymlsummit.org/2019/abstracts/Thakker_Urmish_poster.pdf">Paper</a>]<br>
					<br />
				</ol>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
			<h2>Patents</h2>
			3 patents filed, 2 currently being filed with the USPTO.
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
        		<h2>Professional Service</h2>
				<ul>
					<h3>Program Committee</h3>
  					<li>Conference</li>
					<ul>
						<li><a href="https://community.arm.com/developer/research/b/articles/posts/arm-research-summit-2019-catch-up-and-highlights?utm_source=linkedin&utm_medium=social&utm_campaign=2019_eda-other-research_mk07-3_search_arm&utm_term=research-summit-summary&utm_content=blog">Arm Research Summit (2019)</a> </li>
						<li><a href="https://community.arm.com/developer/research/b/articles/posts/catch-up-on-the-arm-research-summit-2018?_ga=2.188913901.797159494.1571152939-403256752.1565579579">Arm Research Summit (2018)</a></li>
					</ul>
  					<li>Workshops
    					  <ul>
						  <li><a href="https://sites.google.com/view/sustainlp2020/home?authuser=0">EMNLP-SustaiNLP (2020)</a></li>
    					  </ul>
  					</li>
					<h3>Reviewer</h3>
  					<li>Journal</li>
					  <ul>
     						<li>IEEE Journal of Selected Topics in Signal Processing (2019) </li>
						<li><a href="https://dl.acm.org/citation.cfm?id=J967">ACM Journal on Emerging Technologies in Computing Systems (2018)</a></li>
    					  </ul>
					  <li>Workshops
    					  <ul>
						  <li><a href="https://www.emc2-workshop.com/neurips-19">NeurIPS-EMC2 (2019)</a></li>
    					  </ul>
  					</li>
					<h3>Academia/Industry Consortiums</h3>
					<li> TinyML Performance Working Group, <a href="https://mlperf.org/"> MLPerf </a>
					
				</ul>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
		<div class="container">
    		<div class="blurb">
			<h2>Media articles</h2>
				<ul>
					<li> <a href="https://community.arm.com/developer/research/b/articles/posts/skipping-rnn-state-updates-without-retraining-the-original-model">Arm Research Blog on "Skip-RNN work"</a> </li>
					<li> <a href="https://community.arm.com/developer/research/b/articles/posts/tinyml-applications-require-new-network-architectures">Arm Research Blog on why "TinyML Applications Require New Network Architectures"</a> </li>
				</ul>
			<hr>
        		<h2>Media coverage of work</h2>
				<ul>
					<li> <a href="https://community.arm.com/developer/research/b/articles/posts/taking-constrained-ml-to-the-next-level">Arm Research Blog on "Tiny ML work"</a> </li>
				</ul>
			<hr>
    		</div><!-- /.blurb -->
		</div><!-- /.container -->
	</body>
</html>
